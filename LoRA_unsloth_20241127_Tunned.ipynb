{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 最終課題コンペ用 Fine-tuning テンプレート（unsloth）- 改変版\n",
        "# --- 本コードは講義で配布されたサンプルプログラムを改修したものです\n",
        "# --- サンプルコード自体の著作権は 講義主催である東京大学に属します"
      ],
      "metadata": {
        "id": "Vm8J0YW-PXD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------------\n",
        "# パッケージをインストール\n",
        "!pip uninstall unsloth -y\n",
        "!pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\" -qU\n",
        "\n",
        "!pip install --upgrade torch torchvision torchaudio -qU\n",
        "!pip install --upgrade xformers -qU\n",
        "\n",
        "# Install Flash Attention 2 for softcapping support\n",
        "import torch\n",
        "if torch.cuda.get_device_capability()[0] >= 8:\n",
        "    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\" -qU"
      ],
      "metadata": {
        "id": "fU2Yi6mHNz_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------------\n",
        "# パラメータ設定\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# データを下記に配置してください\n",
        "input_path  = \"/content/input/ichikara-instruction-003-001-1.json\"\n",
        "eval_path   = \"/content/eval/elyza-tasks-100-TV_0.jsonl\"\n",
        "result_path = \"/content/results\"\n",
        "\n",
        "# ------------------------------------------------------------------------------------\n",
        "# 学習パラメータを設定\n",
        "model_id = \"llm-jp/llm-jp-3-13b\"\n",
        "new_model_id = \"llm-jp-3-13b-it\"      # Fine-Tuningしたモデルにつけたい名前\n",
        "dtype = None                          # Noneにしておけば自動で設定                                                   +\n",
        "load_in_4bit = True                   # 今回は13Bモデルを扱うためTrue\n",
        "max_seq_length = 512                  # unslothではRoPEをサポートしているのでコンテキスト長は自由に設定可能\n",
        "\n",
        "# パラメータをPack\n",
        "config={\n",
        "    \"model_id\": model_id,\n",
        "    \"learning_rate\": 2e-5,\n",
        "    \"per_device_train_batch_size\": 4,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"num_train_epochs\":3,\n",
        "    \"warmup_steps\": 10,\n",
        "    \"max_steps\": -1,\n",
        "    \"lora_r\": 32,\n",
        "    \"lora_alpha\": 32,\n",
        "    \"lora_dropout\": 0.05,\n",
        "    \"lora_bias\": \"none\",\n",
        "    \"lora_use_rslora\": False,\n",
        "    \"lora_loftq_config\": None,\n",
        "    \"model_max_seq_length\": max_seq_length,\n",
        "    \"model_dtype\": dtype,\n",
        "    \"model_load_in_4bit\": load_in_4bit,\n",
        "    \"seed\": 3407,\n",
        "    \"max_seq_length\": max_seq_length\n",
        "}"
      ],
      "metadata": {
        "id": "SCa8Pz0qN1ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------------\n",
        "# llm-jp/llm-jp-3-13bを4bit量子化のqLoRA設定でロード。\n",
        "from unsloth import FastLanguageModel\n",
        "model_A, tokenizer = FastLanguageModel.from_pretrained(    # FastLanguageModel インスタンスを作成\n",
        "    model_name   = config.model_id,\n",
        "    dtype        = config.model_dtype,\n",
        "    load_in_4bit = config.model_load_in_4bit,\n",
        "    trust_remote_code= True,\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(                  # SFT用のモデルを用意\n",
        "    model_A,\n",
        "    r              = config.lora_r,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha     = config.lora_alpha,\n",
        "    lora_dropout   = config.lora_dropout,\n",
        "    bias           = config.lora_bias,\n",
        "    random_state   = config.seed,\n",
        "    use_rslora     = config.lora_use_rslora,\n",
        "    loftq_config   = config.lora_loftq_config,\n",
        "    max_seq_length = config.max_seq_length,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        ")"
      ],
      "metadata": {
        "id": "N9H5oyfqN1Z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------------\n",
        "# データセットをトレーニングとテストデータセットに分割してロード\n",
        "from datasets import load_dataset\n",
        "train_dataset = load_dataset(\"json\", data_files= input_path, split=\"train[:80%]\" )\n",
        "test_dataset = load_dataset(\"json\", data_files= input_path, split=\"train[80%:]\")\n",
        "\n",
        "#  各データをプロンプトに合わせた形式に合わせる関数「formatting_prompts_func」\n",
        "EOS_TOKEN = tokenizer.eos_token                  # トークナイザーのEOSトークン（文末トークン）\n",
        "prompt = f\"\"\"### 指示\\n{input}\\n### 回答\\n\"\"\"      # 学習時のプロンプトフォーマットの定義\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    input = examples[\"text\"]                          # 入力データ\n",
        "    output = examples[\"output\"]                       # 出力データ\n",
        "    text = prompt.format(input, output) + EOS_TOKEN   # プロンプトの作成\n",
        "    return { \"formatted_text\" : text, }               # 新しいフィールド \"formatted_text\" を返す\n",
        "\n",
        "# 両データセットにPromptフォーマットを適用\n",
        "train_dataset = train_dataset.map( formatting_prompts_func, num_proc= 4 )\n",
        "test_dataset = test_dataset.map( formatting_prompts_func, num_proc= 4 )\n"
      ],
      "metadata": {
        "id": "gAdufQSDN7ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-Atho_PK2WW"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------------\n",
        "# ファインチューニング開始\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments, EarlyStoppingCallback\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# EarlyStoppingCallbackの設定\n",
        "early_stopping_callback = EarlyStoppingCallback(\n",
        "    early_stopping_patience  = 3,         # 改善がない場合に許容するエポック数\n",
        "    early_stopping_threshold = 0.0        # 改善があったとみなす最小の変化量\n",
        ")\n",
        "\n",
        "# Trainerを作成\n",
        "trainer = SFTTrainer(\n",
        "    model              = model,\n",
        "    tokenizer          = tokenizer,\n",
        "    train_dataset      = train_dataset,\n",
        "    eval_dataset       = test_dataset,\n",
        "    max_seq_length     = config.max_seq_length,\n",
        "    dataset_text_field = \"formatted_text\",\n",
        "    packing            = False,\n",
        "    callbacks=[early_stopping_callback],  # コールバックへEarlyStoppingを設定\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = config.per_device_train_batch_size,\n",
        "        gradient_accumulation_steps = config.gradient_accumulation_steps,\n",
        "        num_train_epochs = config.num_train_epochs,\n",
        "        warmup_steps     = config.warmup_steps,\n",
        "        max_steps        = config.max_steps,\n",
        "        learning_rate    = config.learning_rate,\n",
        "        seed             = config.seed,\n",
        "        evaluation_strategy = \"steps\",       # ステップ単位で評価\n",
        "        eval_steps       = 20,               # 20ステップごとに評価\n",
        "        save_strategy    = \"steps\",          # ステップ単位でモデルを保存\n",
        "        save_steps       = 60,               # 60ステップごとに保存\n",
        "        save_total_limit = 3,\n",
        "        load_best_model_at_end = True,        # ベストモデルをロード\n",
        "        metric_for_best_model  = \"eval_loss\", # EarlyStoppingの基準メトリクス/損失を基準に最良モデルを選択\n",
        "        greater_is_better      = False,       # eval_lossは小さい方が良い\n",
        "        output_dir       = \"outputs\",\n",
        "        report_to        = \"wandb\",\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        group_by_length  = True,\n",
        "        logging_steps    = 10,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------------\n",
        "# 学習実行\n",
        "trainer_stats = trainer.train()\n"
      ],
      "metadata": {
        "id": "2BE4EFiWPPVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------------\n",
        "# Elyza-100をDatasetとして読み込む\n",
        "import json\n",
        "eval_datasets = []\n",
        "elyza_tasks_path = eval_path\n",
        "with open(elyza_tasks_path, \"r\") as f:\n",
        "    item = \"\"\n",
        "    for line in f:\n",
        "      line = line.strip()\n",
        "      item += line\n",
        "      if item.endswith(\"}\"):\n",
        "        eval_datasets.append(json.loads(item))\n",
        "        item = \"\"\n"
      ],
      "metadata": {
        "id": "wv5GUiftOqSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ファインチューニングしたモデルを用いて、Elyza-100質問タスクを実行\n",
        "from tqdm import tqdm\n",
        "FastLanguageModel.for_inference(model)\n",
        "model.eval()\n",
        "results = []\n",
        "for dt in tqdm(eval_datasets):\n",
        "  input = dt[\"input\"]\n",
        "  prompt = f\"\"\"### 指示\\n{input}\\n### 回答\\n\"\"\"\n",
        "  inputs = tokenizer([prompt], return_tensors = \"pt\")\n",
        "  outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True, do_sample=False, repetition_penalty=1.2)\n",
        "  prediction = tokenizer.decode(outputs[0], skip_special_tokens=True).split('\\n### 回答')[-1]\n",
        "  results.append({\"task_id\": dt[\"task_id\"], \"input\": input, \"output\": prediction})"
      ],
      "metadata": {
        "id": "4j_IojD-OtYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------------------------------\n",
        "# jsonlで保存\n",
        "jsonl_result_path = result_path + f\"/{new_model_id}_output.jsonl\",\n",
        "with open( jsonl_result_path, 'w', encoding='utf-8') as f:\n",
        "    for result in results:\n",
        "        json.dump(result, f, ensure_ascii=False)\n",
        "        f.write('\\n')\n"
      ],
      "metadata": {
        "id": "i44kGV8UOocd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRAアダプタだけをH保存\n",
        "HF_TOKEN = \"huggingface Access Token\"\n",
        "model.push_to_hub_merged(\n",
        "    new_model_id+\"_lora\",\n",
        "    tokenizer=tokenizer,\n",
        "    save_method=\"lora\",\n",
        "    token=HF_TOKEN,\n",
        "    private=False\n",
        ")"
      ],
      "metadata": {
        "id": "Y2hUOXr1Owkb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}